from producto_loader import load_productos, load_faqs
from rag_engine import build_index
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from langchain_community.llms import Ollama
import os

if not os.path.exists("storage"):
    os.makedirs("storage")

print("ðŸ”„ Cargando productos y FAQs...")
productos_docs = load_productos("info/productos.json")
faqs_docs = load_faqs("info/faqs.json")
all_docs = productos_docs + faqs_docs
print(f"âœ… Documentos cargados: {len(all_docs)}")

print("ðŸ”„ Inicializando embedding y modelo...")
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-large-en-v1.5")
llm = Ollama(model="gemma3:1b")

print("ðŸ”„ Construyendo el Ã­ndice vectorial...")
index = build_index(all_docs, embed_model, llm)

print("ðŸ’¾ Guardando Ã­ndice en disco (storage/)...")
index.storage_context.persist(persist_dir="storage")
print("âœ… Â¡Index guardado listo para usar!")
