# build_index.py
from producto_loader import load_productos
from rag_engine import build_index
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from langchain_community.llms import Ollama
from llama_index.core import StorageContext

import os

if not os.path.exists("storage"):
    os.makedirs("storage")

print("ðŸ”„ Cargando productos...")
documents = load_productos("info/productos.json")
print(f"âœ… Productos cargados: {len(documents)}")

print("ðŸ”„ Inicializando embedding y modelo...")
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-large-en-v1.5")
llm = Ollama(model="gemma3:1b")

print("ðŸ”„ Construyendo el Ã­ndice vectorial (esto sÃ­ puede tardar varios minutos)...")
index = build_index(documents, embed_model, llm)

print("ðŸ’¾ Guardando Ã­ndice en disco (storage/)...")
index.storage_context.persist(persist_dir="storage")
print("âœ… Â¡Index guardado listo para usar!")
